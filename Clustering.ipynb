{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e6f5c9f",
      "metadata": {
        "id": "5e6f5c9f"
      },
      "source": [
        "# Clustering Assignment: 48 Questions with Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5dc5cf9",
      "metadata": {
        "id": "b5dc5cf9"
      },
      "source": [
        "**1. What is unsupervised learning in the context of machine learning?**\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the model learns patterns from unlabelled data. The algorithm tries to find hidden structure, clusters, or associations within the dataset without any target output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b2cc4b",
      "metadata": {
        "id": "96b2cc4b"
      },
      "source": [
        "**2. How does K-Means clustering algorithm work?**\n",
        "\n",
        "K-Means clusters data by initializing 'k' centroids, assigning each point to the nearest centroid, then updating the centroids as the mean of all points in each cluster. This process repeats until the centroids no longer change significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a4c844",
      "metadata": {
        "id": "34a4c844"
      },
      "source": [
        "**3. Explain the concept of a dendrogram in hierarchical clustering?**\n",
        "\n",
        "A dendrogram is a tree-like diagram that shows the arrangement of clusters formed by hierarchical clustering. It illustrates the merging or splitting of clusters at various levels of similarity or distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "429676c7",
      "metadata": {
        "id": "429676c7"
      },
      "source": [
        "**4. What is the main difference between K-Means and Hierarchical Clustering?**\n",
        "\n",
        "K-Means is a partitional clustering method that needs the number of clusters as input, while Hierarchical Clustering builds a hierarchy of clusters and doesn't require specifying the number of clusters beforehand."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d075a90",
      "metadata": {
        "id": "4d075a90"
      },
      "source": [
        "**5. What are the advantages of DBSCAN over K-Means?**\n",
        "\n",
        "DBSCAN can find clusters of arbitrary shapes, handles noise well, and does not require the number of clusters in advance, unlike K-Means which assumes spherical clusters and fixed k value."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1d41608",
      "metadata": {
        "id": "b1d41608"
      },
      "source": [
        "**6. When would you use Silhouette Score in clustering?**\n",
        "\n",
        "Silhouette Score is used to measure how well data points fit within their clusters. It helps evaluate the quality of clustering and decide the optimal number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "648b986f",
      "metadata": {
        "id": "648b986f"
      },
      "source": [
        "**7. What are the limitations of Hierarchical Clustering?**\n",
        "\n",
        "It is computationally expensive for large datasets and sensitive to noise and outliers. Also, once a decision is made to merge or split clusters, it cannot be undone."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a252f843",
      "metadata": {
        "id": "a252f843"
      },
      "source": [
        "**8. Why is feature scaling important in clustering algorithms like K-Means?**\n",
        "\n",
        "Feature scaling ensures that each feature contributes equally to the distance calculations used by clustering algorithms. Without scaling, features with larger ranges can dominate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0935f52",
      "metadata": {
        "id": "a0935f52"
      },
      "source": [
        "**9. How does DBSCAN identify noise points?**\n",
        "\n",
        "DBSCAN labels a point as noise if it has fewer neighbors within a defined radius (eps) than a minimum number of points (min_samples). These points donâ€™t belong to any cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c720e3b1",
      "metadata": {
        "id": "c720e3b1"
      },
      "source": [
        "**10. Define inertia in the context of K-Means?**\n",
        "\n",
        "Inertia is the sum of squared distances between each point and its assigned cluster centroid. It measures how internally coherent the clusters are. Lower inertia means better clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add4dbe7",
      "metadata": {
        "id": "add4dbe7"
      },
      "source": [
        "**11. What is the elbow method in K-Means clustering?**\n",
        "\n",
        "The elbow method involves plotting the inertia against various k values and selecting the 'elbow point' where inertia decreases less sharply. This helps determine the optimal number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e042b9e5",
      "metadata": {
        "id": "e042b9e5"
      },
      "source": [
        "**12. Describe the concept of \"density\" in DBSCAN?**\n",
        "\n",
        "Density in DBSCAN refers to the number of points in a given neighborhood (radius eps). A region is considered dense if it has at least min_samples points within eps distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b05de9",
      "metadata": {
        "id": "44b05de9"
      },
      "source": [
        "**13. Can hierarchical clustering be used on categorical data?**\n",
        "\n",
        "Yes, but it requires using a suitable distance metric for categorical data (like Hamming distance) and might not perform as well as specialized categorical clustering methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9697ca8f",
      "metadata": {
        "id": "9697ca8f"
      },
      "source": [
        "**14. What does a negative Silhouette Score indicate?**\n",
        "\n",
        "A negative Silhouette Score means that the sample is likely placed in the wrong cluster, as it is closer to points in a neighboring cluster than to its own cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e59b414c",
      "metadata": {
        "id": "e59b414c"
      },
      "source": [
        "**15. Explain the term \"linkage criteria\" in hierarchical clustering?**\n",
        "\n",
        "Linkage criteria define how the distance between clusters is calculated. Common types include single, complete, average, and ward linkage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95261b32",
      "metadata": {
        "id": "95261b32"
      },
      "source": [
        "**16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?**\n",
        "\n",
        "Because K-Means assumes clusters are spherical and of similar size and density. It may merge small dense clusters or split large sparse ones incorrectly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088f0b55",
      "metadata": {
        "id": "088f0b55"
      },
      "source": [
        "**17. What are the core parameters in DBSCAN, and how do they influence clustering?**\n",
        "\n",
        "The main parameters are eps (radius of neighborhood) and min_samples (minimum points to form a dense region). They control cluster formation and noise detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa748fce",
      "metadata": {
        "id": "aa748fce"
      },
      "source": [
        "**18. How does K-Means++ improve upon standard K-Means initialization?**\n",
        "\n",
        "K-Means++ initializes centroids in a smarter way by spreading them out, which reduces the chances of poor clustering and speeds up convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c38c070",
      "metadata": {
        "id": "6c38c070"
      },
      "source": [
        "**19. What is agglomerative clustering?**\n",
        "\n",
        "Agglomerative clustering is a bottom-up approach where each point starts in its own cluster, and clusters are merged step by step based on distance metrics until one big cluster is formed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2935583d",
      "metadata": {
        "id": "2935583d"
      },
      "source": [
        "**20. What makes Silhouette Score a better metric than just inertia for model evaluation?**\n",
        "\n",
        "Silhouette Score considers both intra-cluster tightness and inter-cluster separation, making it a more balanced and informative metric than inertia alone."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78fcc9ed",
      "metadata": {
        "id": "78fcc9ed"
      },
      "source": [
        "### Q21. Generate synthetic data with 4 centers using `make_blobs` and apply K-Means clustering. Visualize using a scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b553f173",
      "metadata": {
        "id": "b553f173"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, color='red', marker='X')\n",
        "plt.title(\"K-Means Clustering with 4 Centers\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f67112",
      "metadata": {
        "id": "05f67112"
      },
      "source": [
        "### Q22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea5f4fc",
      "metadata": {
        "id": "2ea5f4fc"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "print(\"First 10 predicted labels:\", labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ed35d1",
      "metadata": {
        "id": "f0ed35d1"
      },
      "source": [
        "### Q23. Generate synthetic data using `make_moons` and apply DBSCAN. Highlight outliers in the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272428e2",
      "metadata": {
        "id": "272428e2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Paired')\n",
        "plt.title(\"DBSCAN on make_moons (outliers shown as -1)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster?"
      ],
      "metadata": {
        "id": "s4HsKTQ0BXC_"
      },
      "id": "s4HsKTQ0BXC_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Print size of each cluster\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(\"Cluster sizes:\", dict(zip(unique, counts)))\n"
      ],
      "metadata": {
        "id": "KzmpcHWsBgPM"
      },
      "id": "KzmpcHWsBgPM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result?"
      ],
      "metadata": {
        "id": "HHsOsKHyBoQ5"
      },
      "id": "HHsOsKHyBoQ5"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "\n",
        "# DBSCAN\n",
        "db = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm')\n",
        "plt.title(\"DBSCAN on make_circles Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nD9cD8A7BpEb"
      },
      "id": "nD9cD8A7BpEb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids?"
      ],
      "metadata": {
        "id": "4VyBf_PGBrrE"
      },
      "id": "4VyBf_PGBrrE"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Scale\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Centroids\n",
        "print(\"Cluster centroids:\\n\", kmeans.cluster_centers_)\n"
      ],
      "metadata": {
        "id": "-yVAX5iSB2aW"
      },
      "id": "-yVAX5iSB2aW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN?"
      ],
      "metadata": {
        "id": "oSky_myjB7qD"
      },
      "id": "oSky_myjB7qD"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
        "\n",
        "# DBSCAN\n",
        "db = DBSCAN(eps=1.2, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1')\n",
        "plt.title(\"DBSCAN on Blobs with Varying Std\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KNREMemjCDT-"
      },
      "id": "KNREMemjCDT-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means?"
      ],
      "metadata": {
        "id": "LCPyKzMWCF6l"
      },
      "id": "LCPyKzMWCF6l"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# PCA to 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"K-Means Clustering on Digits (PCA 2D)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OyFXpuBWCI02"
      },
      "id": "OyFXpuBWCI02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart?"
      ],
      "metadata": {
        "id": "2-Q90Hj-CLAG"
      },
      "id": "2-Q90Hj-CLAG"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Calculate Silhouette Scores\n",
        "scores = []\n",
        "for k in range(2, 6):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    scores.append(score)\n",
        "\n",
        "# Plot\n",
        "plt.bar(range(2, 6), scores, color='orange')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Scores for K = 2 to 5')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gJPjLAdmCRkQ"
      },
      "id": "gJPjLAdmCRkQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage?"
      ],
      "metadata": {
        "id": "sMxvGPeRCUgB"
      },
      "id": "sMxvGPeRCUgB"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Linkage and dendrogram\n",
        "linked = linkage(X, method='average')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linked, labels=iris.target, truncate_mode='level', p=5)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram (Average Linkage)\")\n",
        "plt.xlabel(\"Data Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bv0qhFaoCXSv"
      },
      "id": "Bv0qhFaoCXSv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries?"
      ],
      "metadata": {
        "id": "WCBxxEFpCZjo"
      },
      "id": "WCBxxEFpCZjo"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data with overlapping clusters\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.predict(X)\n",
        "\n",
        "# Plot decision boundaries\n",
        "x_min, x_max = X[:, 0].min()-1, X[:, 0].max()+1\n",
        "y_min, y_max = X[:, 1].min()-1, X[:, 1].max()+1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='red')\n",
        "plt.title(\"K-Means with Decision Boundaries\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ShdDpJ3zCdIp"
      },
      "id": "ShdDpJ3zCdIp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results?"
      ],
      "metadata": {
        "id": "Rf9_CAK0CoW_"
      },
      "id": "Rf9_CAK0CoW_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and reduce\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\n",
        "\n",
        "# DBSCAN\n",
        "db = DBSCAN(eps=5, min_samples=5)\n",
        "labels = db.fit_predict(X_tsne)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"DBSCAN on Digits Dataset (t-SNE Reduced)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Jaxt5DLDCrcW"
      },
      "id": "Jaxt5DLDCrcW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result?"
      ],
      "metadata": {
        "id": "eGeO3NJJCt60"
      },
      "id": "eGeO3NJJCt60"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')\n",
        "plt.title(\"Agglomerative Clustering (Complete Linkage)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X7MLm6xaCxME"
      },
      "id": "X7MLm6xaCxME",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot?"
      ],
      "metadata": {
        "id": "g6Di6x1yCzoO"
      },
      "id": "g6Di6x1yCzoO"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and scale\n",
        "data = load_breast_cancer()\n",
        "X = StandardScaler().fit_transform(data.data)\n",
        "\n",
        "# Inertia for k = 2 to 6\n",
        "inertias = []\n",
        "k_values = range(2, 7)\n",
        "for k in k_values:\n",
        "    km = KMeans(n_clusters=k, random_state=42)\n",
        "    km.fit(X)\n",
        "    inertias.append(km.inertia_)\n",
        "\n",
        "# Plot\n",
        "plt.plot(k_values, inertias, marker='o')\n",
        "plt.title(\"Inertia for K = 2 to 6\")\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hkKxLl8WC2xS"
      },
      "id": "hkKxLl8WC2xS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage?"
      ],
      "metadata": {
        "id": "8QiPUwHcC-Fi"
      },
      "id": "8QiPUwHcC-Fi"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "\n",
        "# Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='cool')\n",
        "plt.title(\"Agglomerative Clustering on Circles (Single Linkage)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "woU77Q-8C-sD"
      },
      "id": "woU77Q-8C-sD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)?"
      ],
      "metadata": {
        "id": "DL7zmxM7DBh6"
      },
      "id": "DL7zmxM7DBh6"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load and scale\n",
        "data = load_wine()\n",
        "X = StandardScaler().fit_transform(data.data)\n",
        "\n",
        "# DBSCAN\n",
        "db = DBSCAN(eps=1.5, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "# Count clusters (excluding noise)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters (excluding noise):\", n_clusters)\n"
      ],
      "metadata": {
        "id": "D5XCL1HjDE6I"
      },
      "id": "D5XCL1HjDE6I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points?"
      ],
      "metadata": {
        "id": "7cST2k-kDIAF"
      },
      "id": "7cST2k-kDIAF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='red', marker='x')\n",
        "plt.title(\"K-Means Clustering with Centers\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2PRo9lIODKxi"
      },
      "id": "2PRo9lIODKxi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise?"
      ],
      "metadata": {
        "id": "wAz96a-cDM4_"
      },
      "id": "wAz96a-cDM4_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load and scale\n",
        "X = load_iris().data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# DBSCAN\n",
        "db = DBSCAN(eps=0.8, min_samples=5)\n",
        "labels = db.fit_predict(X_scaled)\n",
        "\n",
        "# Count noise (-1)\n",
        "n_noise = np.sum(labels == -1)\n",
        "print(\"Number of noise samples:\", n_noise)\n"
      ],
      "metadata": {
        "id": "t9gBQhR5DPfK"
      },
      "id": "t9gBQhR5DPfK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result?\n"
      ],
      "metadata": {
        "id": "49S56iqCDWfo"
      },
      "id": "49S56iqCDWfo"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set2')\n",
        "plt.title(\"K-Means on make_moons (Non-linear Clusters)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KoMjCa28DbJ0"
      },
      "id": "KoMjCa28DbJ0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot?"
      ],
      "metadata": {
        "id": "lGdsxyUoDc9w"
      },
      "id": "lGdsxyUoDc9w"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Load and reduce\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# 3D Plot\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='Spectral')\n",
        "ax.set_title(\"3D PCA + KMeans on Digits\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dfWpXzPtDfxK"
      },
      "id": "dfWpXzPtDfxK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering?"
      ],
      "metadata": {
        "id": "kAVnUb4RDiLF"
      },
      "id": "kAVnUb4RDiLF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Data\n",
        "X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Silhouette Score\n",
        "score = silhouette_score(X, labels)\n",
        "print(\"Silhouette Score:\", score)\n"
      ],
      "metadata": {
        "id": "ZQBUKtY8DnV_"
      },
      "id": "ZQBUKtY8DnV_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D?"
      ],
      "metadata": {
        "id": "eHEd5OV9Do7v"
      },
      "id": "eHEd5OV9Do7v"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and scale\n",
        "data = load_breast_cancer()\n",
        "X = StandardScaler().fit_transform(data.data)\n",
        "\n",
        "# PCA\n",
        "X_pca = PCA(n_components=2).fit_transform(X)\n",
        "\n",
        "# Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=2)\n",
        "labels = agg.fit_predict(X_pca)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='coolwarm')\n",
        "plt.title(\"Agglomerative Clustering on Breast Cancer (2D PCA)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "awTFSM39Ds32"
      },
      "id": "awTFSM39Ds32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side?"
      ],
      "metadata": {
        "id": "__Vm_cuWDvXl"
      },
      "id": "__Vm_cuWDvXl"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42).fit_predict(X)\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5).fit_predict(X)\n",
        "\n",
        "# Plot side by side\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans, cmap='Set2')\n",
        "plt.title(\"K-Means Clustering\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=dbscan, cmap='Set1')\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PqH9rczxD25Q"
      },
      "id": "PqH9rczxD25Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering?"
      ],
      "metadata": {
        "id": "WLukhFFvD5KF"
      },
      "id": "WLukhFFvD5KF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "X = load_iris().data\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Silhouette Coefficients\n",
        "sil_values = silhouette_samples(X, labels)\n",
        "\n",
        "# Plot\n",
        "plt.bar(range(len(X)), sil_values)\n",
        "plt.title(\"Silhouette Coefficient for Each Sample (Iris)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Silhouette Value\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Kly1LB5fD8QP"
      },
      "id": "Kly1LB5fD8QP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters?"
      ],
      "metadata": {
        "id": "UBP5uUeqD-TT"
      },
      "id": "UBP5uUeqD-TT"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "\n",
        "# Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Accent')\n",
        "plt.title(\"Agglomerative Clustering with Average Linkage\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RcrTRy3NEAwc"
      },
      "id": "RcrTRy3NEAwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)?"
      ],
      "metadata": {
        "id": "KJGkh0iuECik"
      },
      "id": "KJGkh0iuECik"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df = df.iloc[:, :4]  # First 4 features\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(df)\n",
        "\n",
        "# Pairplot\n",
        "sns.pairplot(df, hue='Cluster', diag_kind='kde')\n"
      ],
      "metadata": {
        "id": "_q3OohpcEGbU"
      },
      "id": "_q3OohpcEGbU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count?"
      ],
      "metadata": {
        "id": "f4Pc86-MESAx"
      },
      "id": "f4Pc86-MESAx"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.5, random_state=42)\n",
        "\n",
        "# DBSCAN\n",
        "db = DBSCAN(eps=1.2, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "# Count clusters and noise\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = np.sum(labels == -1)\n",
        "\n",
        "print(\"Clusters found:\", n_clusters)\n",
        "print(\"Noise points:\", n_noise)\n"
      ],
      "metadata": {
        "id": "M1XzXOyWEWKE"
      },
      "id": "M1XzXOyWEWKE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters?"
      ],
      "metadata": {
        "id": "iXBeTCfEEYO_"
      },
      "id": "iXBeTCfEEYO_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and reduce\n",
        "X = load_digits().data\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\n",
        "\n",
        "# Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=10)\n",
        "labels = agg.fit_predict(X_tsne)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"Agglomerative Clustering on Digits (t-SNE Reduced)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0AObhlIeEbik"
      },
      "id": "0AObhlIeEbik",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}