{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f5c9f",
   "metadata": {},
   "source": [
    "# Clustering Assignment: 48 Questions with Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc5cf9",
   "metadata": {},
   "source": [
    "**1. What is unsupervised learning in the context of machine learning?**\n",
    "\n",
    "Unsupervised learning is a type of machine learning where the model learns patterns from unlabelled data. The algorithm tries to find hidden structure, clusters, or associations within the dataset without any target output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2cc4b",
   "metadata": {},
   "source": [
    "**2. How does K-Means clustering algorithm work?**\n",
    "\n",
    "K-Means clusters data by initializing 'k' centroids, assigning each point to the nearest centroid, then updating the centroids as the mean of all points in each cluster. This process repeats until the centroids no longer change significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4c844",
   "metadata": {},
   "source": [
    "**3. Explain the concept of a dendrogram in hierarchical clustering?**\n",
    "\n",
    "A dendrogram is a tree-like diagram that shows the arrangement of clusters formed by hierarchical clustering. It illustrates the merging or splitting of clusters at various levels of similarity or distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429676c7",
   "metadata": {},
   "source": [
    "**4. What is the main difference between K-Means and Hierarchical Clustering?**\n",
    "\n",
    "K-Means is a partitional clustering method that needs the number of clusters as input, while Hierarchical Clustering builds a hierarchy of clusters and doesn't require specifying the number of clusters beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d075a90",
   "metadata": {},
   "source": [
    "**5. What are the advantages of DBSCAN over K-Means?**\n",
    "\n",
    "DBSCAN can find clusters of arbitrary shapes, handles noise well, and does not require the number of clusters in advance, unlike K-Means which assumes spherical clusters and fixed k value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d41608",
   "metadata": {},
   "source": [
    "**6. When would you use Silhouette Score in clustering?**\n",
    "\n",
    "Silhouette Score is used to measure how well data points fit within their clusters. It helps evaluate the quality of clustering and decide the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b986f",
   "metadata": {},
   "source": [
    "**7. What are the limitations of Hierarchical Clustering?**\n",
    "\n",
    "It is computationally expensive for large datasets and sensitive to noise and outliers. Also, once a decision is made to merge or split clusters, it cannot be undone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252f843",
   "metadata": {},
   "source": [
    "**8. Why is feature scaling important in clustering algorithms like K-Means?**\n",
    "\n",
    "Feature scaling ensures that each feature contributes equally to the distance calculations used by clustering algorithms. Without scaling, features with larger ranges can dominate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0935f52",
   "metadata": {},
   "source": [
    "**9. How does DBSCAN identify noise points?**\n",
    "\n",
    "DBSCAN labels a point as noise if it has fewer neighbors within a defined radius (eps) than a minimum number of points (min_samples). These points donâ€™t belong to any cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720e3b1",
   "metadata": {},
   "source": [
    "**10. Define inertia in the context of K-Means?**\n",
    "\n",
    "Inertia is the sum of squared distances between each point and its assigned cluster centroid. It measures how internally coherent the clusters are. Lower inertia means better clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4dbe7",
   "metadata": {},
   "source": [
    "**11. What is the elbow method in K-Means clustering?**\n",
    "\n",
    "The elbow method involves plotting the inertia against various k values and selecting the 'elbow point' where inertia decreases less sharply. This helps determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042b9e5",
   "metadata": {},
   "source": [
    "**12. Describe the concept of \"density\" in DBSCAN?**\n",
    "\n",
    "Density in DBSCAN refers to the number of points in a given neighborhood (radius eps). A region is considered dense if it has at least min_samples points within eps distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b05de9",
   "metadata": {},
   "source": [
    "**13. Can hierarchical clustering be used on categorical data?**\n",
    "\n",
    "Yes, but it requires using a suitable distance metric for categorical data (like Hamming distance) and might not perform as well as specialized categorical clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697ca8f",
   "metadata": {},
   "source": [
    "**14. What does a negative Silhouette Score indicate?**\n",
    "\n",
    "A negative Silhouette Score means that the sample is likely placed in the wrong cluster, as it is closer to points in a neighboring cluster than to its own cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b414c",
   "metadata": {},
   "source": [
    "**15. Explain the term \"linkage criteria\" in hierarchical clustering?**\n",
    "\n",
    "Linkage criteria define how the distance between clusters is calculated. Common types include single, complete, average, and ward linkage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95261b32",
   "metadata": {},
   "source": [
    "**16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?**\n",
    "\n",
    "Because K-Means assumes clusters are spherical and of similar size and density. It may merge small dense clusters or split large sparse ones incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f0b55",
   "metadata": {},
   "source": [
    "**17. What are the core parameters in DBSCAN, and how do they influence clustering?**\n",
    "\n",
    "The main parameters are eps (radius of neighborhood) and min_samples (minimum points to form a dense region). They control cluster formation and noise detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa748fce",
   "metadata": {},
   "source": [
    "**18. How does K-Means++ improve upon standard K-Means initialization?**\n",
    "\n",
    "K-Means++ initializes centroids in a smarter way by spreading them out, which reduces the chances of poor clustering and speeds up convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38c070",
   "metadata": {},
   "source": [
    "**19. What is agglomerative clustering?**\n",
    "\n",
    "Agglomerative clustering is a bottom-up approach where each point starts in its own cluster, and clusters are merged step by step based on distance metrics until one big cluster is formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935583d",
   "metadata": {},
   "source": [
    "**20. What makes Silhouette Score a better metric than just inertia for model evaluation?**\n",
    "\n",
    "Silhouette Score considers both intra-cluster tightness and inter-cluster separation, making it a more balanced and informative metric than inertia alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fcc9ed",
   "metadata": {},
   "source": [
    "### Q21. Generate synthetic data with 4 centers using `make_blobs` and apply K-Means clustering. Visualize using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, color='red', marker='X')\n",
    "plt.title(\"K-Means Clustering with 4 Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f67112",
   "metadata": {},
   "source": [
    "### Q22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "labels = agg.fit_predict(X)\n",
    "\n",
    "print(\"First 10 predicted labels:\", labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed35d1",
   "metadata": {},
   "source": [
    "### Q23. Generate synthetic data using `make_moons` and apply DBSCAN. Highlight outliers in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272428e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Paired')\n",
    "plt.title(\"DBSCAN on make_moons (outliers shown as -1)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
